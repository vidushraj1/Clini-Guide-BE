{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNymb67zdNJk"
      },
      "outputs": [],
      "source": [
        "# @title 1. Install Libraries and Login\n",
        "\n",
        "# Install necessary packages for transformers, training, and Hugging Face integration\n",
        "!pip install -U -q transformers datasets accelerate peft bitsandbytes trl torch huggingface_hub ipywidgets Jinja2 tqdm openai\n",
        "\n",
        "# Suppress warning messages to reduce clutter in output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Core Python and ML imports\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import time\n",
        "import logging\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Hugging Face & PEFT related imports for model loading and training\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        ")\n",
        "\n",
        "# Hugging Face auth + OpenAI integration\n",
        "from huggingface_hub import notebook_login, HfApi\n",
        "from google.colab import userdata, files\n",
        "import openai\n",
        "\n",
        "# Configure logging for better tracking/debugging of model and API events\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"huggingface_hub\").setLevel(logging.INFO)\n",
        "logging.getLogger(\"openai\").setLevel(logging.INFO)\n",
        "\n",
        "print(\"--- Hugging Face Login (Optional - Needed for Pushing Adapters) ---\")\n",
        "try:\n",
        "    # Try to fetch HF token from Colab Secrets\n",
        "    hf_token = userdata.get('HF_HUB_TOKEN')\n",
        "    if hf_token:\n",
        "        print(\"Using HF token from Colab Secrets.\")\n",
        "        from huggingface_hub import login\n",
        "        login(token=hf_token)\n",
        "    else:\n",
        "        # Fallback to interactive login if no token in secrets\n",
        "        print(\"HF_HUB_TOKEN secret not found. Using interactive login:\")\n",
        "        notebook_login()\n",
        "except Exception as e:\n",
        "    print(f\"HF login failed: {e}.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"--- Configuring OpenAI Client ---\")\n",
        "try:\n",
        "    # Try loading OpenAI key securely from secrets\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    if not openai_api_key:\n",
        "        raise ValueError(\"OpenAI API Key not found in Colab Secrets. Please add it under the name 'OPENAI_API_KEY'.\")\n",
        "    openai.api_key = openai_api_key\n",
        "\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(api_key=openai.api_key)\n",
        "    print(\"OpenAI client configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR setting up OpenAI Client: {e}\")\n",
        "    client = None\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59oRGsR8oT-q"
      },
      "outputs": [],
      "source": [
        "# @title 2. Configuration Parameters\n",
        "\n",
        "# Model identifiers for student (distilled) and teacher (OpenAI GPT-4) models\n",
        "STUDENT_MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "OPENAI_TEACHER_MODEL = \"gpt-4-turbo\"\n",
        "\n",
        "# Paths for saving output models, checkpoints, and input datasets\n",
        "LOCAL_OUTPUT_DIR = \"/content/medQwen-0.5b-distilled-adapters\"\n",
        "CHECKPOINT_DIR = \"/content/distill_openai_checkpoints\"\n",
        "HUB_REPO_ID_DISTILL = \"Vidush25/medQwen-0.5b-distilled-adapters\"\n",
        "KB_DATA_JSON_PATH = \"/content/KB_data.json\"\n",
        "BASE_DATASET_PATH = \"/content/base_medical_dialogues.json\"\n",
        "KB_DATA_SIM_PATH = \"/content/KB_data_sim.json\"\n",
        "\n",
        "# Training hyperparameters\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM_STEPS = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "\n",
        "# Knowledge distillation parameters\n",
        "DISTILL_SAMPLE_LIMIT = 30\n",
        "OPENAI_MAX_TOKENS = 300\n",
        "OPENAI_TEMPERATURE = 0.5\n",
        "\n",
        "# LoRA configuration for parameter-efficient fine-tuning\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]  # Layer targets for LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_Q9fa9gokWk"
      },
      "outputs": [],
      "source": [
        "# @title 3. Load KB Sim Data & Prompt Dataset\n",
        "\n",
        "# Load or construct simulation data from a medical Knowledge (KB)\n",
        "KB_sim_lookup = {}\n",
        "\n",
        "def load_or_create_KB_sim(KB_json_path):\n",
        "    \"\"\"\n",
        "    Load precomputed KB simulation or generate it from raw JSON.\n",
        "    This lookup helps relate symptoms to diseases and handle med interactions.\n",
        "    \"\"\"\n",
        "    KB_sim_lookup_func = {}\n",
        "    if os.path.exists(KB_DATA_SIM_PATH):\n",
        "        try:\n",
        "            with open(KB_DATA_SIM_PATH, 'r') as f:\n",
        "                KB_sim_lookup_func = json.load(f)\n",
        "            logger.info(f\"Loaded pre-processed KB simulation from {KB_DATA_SIM_PATH}\")\n",
        "            return KB_sim_lookup_func\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error loading KB sim file {KB_DATA_SIM_PATH}: {e}. Will attempt to generate.\")\n",
        "\n",
        "    if not os.path.exists(KB_json_path):\n",
        "        logger.error(f\"Raw KB data file '{KB_json_path}' not found. Cannot create KB simulation.\")\n",
        "        return {}\n",
        "\n",
        "    # Generate KB simulation from scratch\n",
        "    try:\n",
        "        with open(KB_json_path, 'r') as f:\n",
        "            KB_raw_data = json.load(f)\n",
        "\n",
        "        for entry in KB_raw_data:\n",
        "            disease = entry.get('disease')\n",
        "            if not disease:\n",
        "                continue\n",
        "\n",
        "            # Map each symptom to related diseases\n",
        "            for symptom in entry.get('symptoms', []):\n",
        "                s_lower = symptom.lower().strip()\n",
        "                if s_lower:\n",
        "                    KB_sim_lookup_func.setdefault(s_lower, []).append(disease)\n",
        "\n",
        "            # Prepare medication-related data: contraindications and interactions\n",
        "            med_data_map = KB_sim_lookup_func.setdefault('_medications', {})\n",
        "            for med_info in entry.get(\"medications\", []):\n",
        "                med_name = med_info.get(\"name\")\n",
        "                if med_name and med_name not in med_data_map:\n",
        "                    med_data_map[med_name] = {\"contraindications\": set(), \"interactions\": set()}\n",
        "            for contra in entry.get(\"contraindications\", []):\n",
        "                med_name = contra.get(\"medication\")\n",
        "                condition = contra.get(\"condition\")\n",
        "                if med_name in med_data_map and condition:\n",
        "                    med_data_map[med_name][\"contraindications\"].add(condition)\n",
        "            for interact in entry.get(\"interactions\", []):\n",
        "                m1 = interact.get(\"med1\")\n",
        "                m2 = interact.get(\"med2\")\n",
        "                if m1 in med_data_map and m2:\n",
        "                    med_data_map[m1][\"interactions\"].add(m2)\n",
        "                if m2 in med_data_map and m1:\n",
        "                    med_data_map[m2][\"interactions\"].add(m1)\n",
        "\n",
        "        # Convert sets to lists for JSON serialization\n",
        "        for key in KB_sim_lookup_func:\n",
        "            if key != '_medications':\n",
        "                KB_sim_lookup_func[key] = list(set(KB_sim_lookup_func[key]))\n",
        "        if '_medications' in KB_sim_lookup_func:\n",
        "            for med_data in KB_sim_lookup_func['_medications'].values():\n",
        "                med_data[\"contraindications\"] = list(med_data[\"contraindications\"])\n",
        "                med_data[\"interactions\"] = list(med_data[\"interactions\"])\n",
        "\n",
        "        logger.info(f\"Generated KB sim lookup. Symptoms: {len([k for k in KB_sim_lookup_func if k != '_medications'])}. Meds: {len(KB_sim_lookup_func.get('_medications', {}))}.\")\n",
        "\n",
        "        # Save for future use\n",
        "        with open(KB_DATA_SIM_PATH, 'w') as f:\n",
        "            json.dump(KB_sim_lookup_func, f, indent=2)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing KB data for simulation: {e}\", exc_info=True)\n",
        "        return {}\n",
        "\n",
        "    return KB_sim_lookup_func\n",
        "\n",
        "KB_sim_lookup = load_or_create_KB_sim(KB_DATA_JSON_PATH)\n",
        "if not KB_sim_lookup:\n",
        "    logger.warning(\"KB Simulation lookup is empty. Context generation and simulated verifiers will be limited.\")\n",
        "else:\n",
        "    logger.info(\"KB Simulation data loaded/generated successfully.\")\n",
        "\n",
        "# Function to create a teacher prompt using context from the KB\n",
        "def create_teacher_prompt(input_text, KB_sim_lookup):\n",
        "    \"\"\"\n",
        "    Builds a system prompt that includes KB context if symptoms match known conditions.\n",
        "    \"\"\"\n",
        "    matched_context = []\n",
        "    for symptom in KB_sim_lookup.keys():\n",
        "        if symptom != '_medications' and symptom in input_text.lower():\n",
        "            diseases = KB_sim_lookup[symptom]\n",
        "            matched_context.append(f\"Symptom '{symptom}' may indicate: {', '.join(diseases)}.\")\n",
        "    context_str = \"\\n\".join(matched_context) if matched_context else \"No additional context available.\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"Patient symptoms: {input_text}\\n\"\n",
        "        f\"Knowledge Graph Context:\\n{context_str}\\n\\n\"\n",
        "        \"Please provide a detailed consultation. Include a step-by-step reasoning \"\n",
        "        \"in a <think> block and finish with a final assessment in an <answer> block.\"\n",
        "    )\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki5BVhXkotvm"
      },
      "outputs": [],
      "source": [
        "# @title 4. Data Generation (Using OpenAI API as Teacher)\n",
        "\n",
        "def get_openai_teacher_response(prompt: str, retries=2, delay=5) -> str:\n",
        "    \"\"\"Send prompt to OpenAI Chat API and return response text; includes basic retry logic.\"\"\"\n",
        "    if not client:\n",
        "        logger.error(\"OpenAI client not initialized. Cannot make API call.\")\n",
        "        return None\n",
        "\n",
        "    # Structure messages with role-based context to guide the OpenAI assistant\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert medical AI assistant providing detailed consultations. Reason step-by-step using a <think> block. Provide your final assessment, recommendation, or next question in an <answer> block. Be factually accurate and concise.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            # Call OpenAI API with chat-completion format\n",
        "            response = client.chat.completions.create(\n",
        "                model=OPENAI_TEACHER_MODEL,\n",
        "                messages=messages,\n",
        "                max_tokens=OPENAI_MAX_TOKENS,\n",
        "                temperature=OPENAI_TEMPERATURE,\n",
        "                n=1,\n",
        "                stop=None\n",
        "            )\n",
        "            # Check and return response content\n",
        "            if response.choices and len(response.choices) > 0:\n",
        "                content = response.choices[0].message.content\n",
        "                if content:\n",
        "                    logger.debug(f\"OpenAI response received (attempt {attempt+1}). Length: {len(content)}\")\n",
        "                    return content.strip()\n",
        "                else:\n",
        "                    logger.warning(f\"OpenAI response empty (attempt {attempt+1}).\")\n",
        "            else:\n",
        "                logger.warning(f\"Invalid response structure (attempt {attempt+1}). Response: {response}\")\n",
        "\n",
        "            # Retry if not final attempt\n",
        "            if attempt < retries:\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                return \"[ERROR: OpenAI returned empty content]\"\n",
        "\n",
        "        # Handle rate limiting and retry\n",
        "        except openai.RateLimitError as e:\n",
        "            logger.warning(f\"OpenAI Rate Limit hit (attempt {attempt+1}/{retries+1}). Waiting {delay}s. Error: {e}\")\n",
        "            if attempt < retries:\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                logger.error(\"OpenAI Rate Limit exceeded after retries.\")\n",
        "                return f\"[ERROR: OpenAI Rate Limit: {e}]\"\n",
        "\n",
        "        # Handle other API-level failures\n",
        "        except openai.APIError as e:\n",
        "            logger.error(f\"OpenAI API Error (attempt {attempt+1}/{retries+1}): {e}\", exc_info=True)\n",
        "            if attempt < retries:\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                return f\"[ERROR: OpenAI API Error: {e}]\"\n",
        "\n",
        "        # Catch-all for unexpected errors\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error calling OpenAI API (attempt {attempt+1}): {e}\", exc_info=True)\n",
        "            return f\"[ERROR: Unexpected OpenAI call failure: {e}]\"\n",
        "\n",
        "    return None\n",
        "\n",
        "def prepare_distillation_data_openai(base_dataset_path, KB_sim_lookup, sample_limit):\n",
        "    \"\"\"Loads base data, creates prompts, and calls the OpenAI API to get teacher outputs.\"\"\"\n",
        "    logger.info(\"--- Generating Distillation Data using OpenAI Teacher API ---\")\n",
        "    distill_data_list = []\n",
        "    prompts_generated = 0\n",
        "\n",
        "    base_data = []\n",
        "    if not os.path.exists(base_dataset_path):\n",
        "        # Use fallback sample if file is missing\n",
        "        logger.error(f\"Base dataset not found: '{base_dataset_path}'. Using dummy entry.\")\n",
        "        base_data = [{\"input\": \"Cough and fever.\"}]\n",
        "    else:\n",
        "        try:\n",
        "            with open(base_dataset_path, 'r') as f:\n",
        "                base_data = json.load(f)\n",
        "            logger.info(f\"Loaded {len(base_data)} base examples.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading base dataset: {e}\")\n",
        "            base_data = [{\"input\": \"Error loading.\"}]\n",
        "\n",
        "    # Limit the number of samples to avoid exhausting API quota\n",
        "    base_data = base_data[:sample_limit]\n",
        "    logger.info(f\"Generating teacher outputs for {len(base_data)} samples using {OPENAI_TEACHER_MODEL}...\")\n",
        "\n",
        "    for i, example in enumerate(tqdm(base_data, desc=\"Generating Teacher Outputs\")):\n",
        "        # Try multiple common keys to get the input\n",
        "        input_text = example.get('input') or example.get('query') or example.get('question')\n",
        "        if not input_text or not isinstance(input_text, str):\n",
        "            logger.warning(f\"Skipping invalid base example {i}\")\n",
        "            continue\n",
        "\n",
        "        # Generate OpenAI-friendly prompt using KB context\n",
        "        teacher_prompt_text = create_teacher_prompt(input_text, KB_sim_lookup)\n",
        "\n",
        "        # Call OpenAI teacher to get response\n",
        "        teacher_output_text = get_openai_teacher_response(teacher_prompt_text)\n",
        "        if not teacher_output_text or teacher_output_text.startswith(\"[ERROR\"):\n",
        "            logger.error(f\"Failed to get valid teacher response for sample {i}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Store prompt-response pair for student training\n",
        "        student_prompt = create_teacher_prompt(input_text, KB_sim_lookup)\n",
        "        distill_data_list.append({\n",
        "            \"input\": student_prompt,\n",
        "            \"output\": teacher_output_text\n",
        "        })\n",
        "\n",
        "        prompts_generated += 1\n",
        "        time.sleep(0.5)  # Be polite to the API and avoid hitting rate limits\n",
        "\n",
        "    logger.info(f\"Finished generation. Prepared {prompts_generated} distillation data pairs.\")\n",
        "\n",
        "    if not distill_data_list:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        return Dataset.from_list(distill_data_list)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to create Dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "# Trigger distillation data generation if API client is available\n",
        "distillation_dataset = None\n",
        "if client:\n",
        "    distillation_dataset = prepare_distillation_data_openai(\n",
        "        BASE_DATASET_PATH,\n",
        "        KB_sim_lookup,\n",
        "        sample_limit=DISTILL_SAMPLE_LIMIT\n",
        "    )\n",
        "else:\n",
        "    logger.error(\"OpenAI client not available. Cannot generate distillation data.\")\n",
        "\n",
        "# Show one example to confirm data format is valid\n",
        "if distillation_dataset:\n",
        "    print(\"\\n--- Sample Distillation Data Entry ---\")\n",
        "    print(\"Input Prompt:\\n\", distillation_dataset[0]['input'])\n",
        "    print(\"\\nTarget Output:\\n\", distillation_dataset[0]['output'])\n",
        "else:\n",
        "    print(\"\\nERROR: No distillation data generated to proceed.\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oTG0BVDo5F1"
      },
      "outputs": [],
      "source": [
        "# @title 5. Load Student Model and Prepare for Training\n",
        "\n",
        "student_model_peft = None\n",
        "student_tokenizer = None\n",
        "\n",
        "# Proceed only if distillation data exists\n",
        "if distillation_dataset and len(distillation_dataset) > 0:\n",
        "    print(f\"\\n--- Loading Student Model: {STUDENT_MODEL_ID} ---\")\n",
        "    try:\n",
        "        # Quantization config to load model in 4-bit precision for memory efficiency\n",
        "        bnb_config_student = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        )\n",
        "\n",
        "        # Load base student model using quantization + automatic device placement\n",
        "        student_model = AutoModelForCausalLM.from_pretrained(\n",
        "            STUDENT_MODEL_ID,\n",
        "            quantization_config=bnb_config_student,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.bfloat16\n",
        "        )\n",
        "\n",
        "        # Load tokenizer and ensure padding is handled correctly\n",
        "        student_tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL_ID, trust_remote_code=True)\n",
        "        if student_tokenizer.pad_token is None:\n",
        "            student_tokenizer.pad_token = student_tokenizer.eos_token\n",
        "            student_tokenizer.padding_side = \"left\"\n",
        "            student_model.config.pad_token_id = student_tokenizer.eos_token_id\n",
        "            logger.info(\"Set student pad token and padding side.\")\n",
        "\n",
        "        # Disable caching to support gradient checkpointing during training\n",
        "        student_model.config.use_cache = False\n",
        "\n",
        "        # Prepare model for 4-bit training with gradient checkpointing\n",
        "        student_model = prepare_model_for_kbit_training(student_model, use_gradient_checkpointing=True)\n",
        "\n",
        "        # Setup LoRA config for parameter-efficient fine-tuning\n",
        "        lora_config_distill = LoraConfig(\n",
        "            r=LORA_R,\n",
        "            lora_alpha=LORA_ALPHA,\n",
        "            lora_dropout=LORA_DROPOUT,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "            target_modules=LORA_TARGET_MODULES\n",
        "        )\n",
        "\n",
        "        # Wrap student model with LoRA adapters\n",
        "        student_model_peft = get_peft_model(student_model, lora_config_distill)\n",
        "        student_model_peft.print_trainable_parameters()  # Print a summary for debugging/logging\n",
        "        logger.info(\"Student model loaded and prepared.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load/prepare student model: {e}\", exc_info=True)\n",
        "        student_model_peft = None\n",
        "else:\n",
        "    logger.error(\"Cannot load student model - no distillation data.\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwdBD4aEo_9w"
      },
      "outputs": [],
      "source": [
        "# @title 6. Define Data Collator and Preprocessing Function\n",
        "\n",
        "def preprocess_for_distill(examples, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Tokenizes inputs (prompt + teacher output) and sets the prompt tokens in the labels to -100.\n",
        "    This version pads every sequence to max_length.\n",
        "    \"\"\"\n",
        "    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
        "\n",
        "    for prompt, target_output in zip(examples[\"input\"], examples[\"output\"]):\n",
        "        # Combine prompt and response, then tokenize the full sequence\n",
        "        full_text = prompt + target_output + tokenizer.eos_token\n",
        "        tokenized_full = tokenizer(full_text, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        # Tokenize the prompt separately to find how many initial tokens to mask in the labels\n",
        "        prompt_tokens = tokenizer(prompt, truncation=True)['input_ids']\n",
        "        prompt_len = len(prompt_tokens)\n",
        "\n",
        "        # Clone input IDs and mask out the prompt portion from contributing to the loss\n",
        "        labels = tokenized_full[\"input_ids\"].copy()\n",
        "        for i in range(prompt_len - 1):  # Leave EOS token as part of the loss region\n",
        "            labels[i] = -100  # Mask these positions in the loss computation\n",
        "\n",
        "        # Append processed input, attention mask, and labels\n",
        "        model_inputs[\"input_ids\"].append(tokenized_full[\"input_ids\"])\n",
        "        model_inputs[\"attention_mask\"].append(tokenized_full[\"attention_mask\"])\n",
        "        model_inputs[\"labels\"].append(labels)\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Run preprocessing over the full dataset\n",
        "tokenized_dataset = None\n",
        "if distillation_dataset and student_tokenizer:\n",
        "    logger.info(\"Tokenizing dataset for distillation...\")\n",
        "    try:\n",
        "        tokenized_dataset = distillation_dataset.map(\n",
        "            lambda examples: preprocess_for_distill(examples, student_tokenizer, MAX_SEQ_LENGTH),\n",
        "            batched=True,\n",
        "            remove_columns=distillation_dataset.column_names  # Clean up extraneous keys\n",
        "        )\n",
        "        logger.info(f\"Dataset tokenized. Final size: {len(tokenized_dataset)}\")\n",
        "\n",
        "        # Show a quick preview of the first tokenized sequence\n",
        "        if len(tokenized_dataset) > 0:\n",
        "            print(\"\\n--- Sample Tokenized Entry (Input IDs) ---\")\n",
        "            print(tokenized_dataset[0]['input_ids'][:60], \"...\")\n",
        "        else:\n",
        "            logger.warning(\"Tokenized dataset is empty.\")\n",
        "            tokenized_dataset = None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to tokenize dataset: {e}\", exc_info=True)\n",
        "else:\n",
        "    logger.warning(\"Skipping tokenization.\")\n",
        "\n",
        "# Define data collator for language modeling loss (no masked LM)\n",
        "data_collator = None\n",
        "if student_tokenizer and student_model_peft:\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=student_tokenizer, mlm=False)\n",
        "    logger.info(\"Data collator defined using DataCollatorForLanguageModeling.\")\n",
        "else:\n",
        "    logger.warning(\"Skipping data collator definition.\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxdKxnQipJmL"
      },
      "outputs": [],
      "source": [
        "# @title 7. Configure and Run Training\n",
        "\n",
        "trainer = None\n",
        "print(\"\\n--- Configuring Distillation Training ---\")\n",
        "\n",
        "# Ensure all necessary components are available before proceeding\n",
        "if tokenized_dataset and student_model_peft and data_collator and student_tokenizer and len(tokenized_dataset) > 0:\n",
        "    try:\n",
        "        # Define training hyperparameters and logging behavior\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=CHECKPOINT_DIR,\n",
        "            num_train_epochs=NUM_EPOCHS,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            logging_dir=f\"{CHECKPOINT_DIR}/logs\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=max(1, len(tokenized_dataset) // (BATCH_SIZE * GRAD_ACCUM_STEPS * 10)),  # log every ~10% of an epoch\n",
        "            save_strategy=\"epoch\",\n",
        "            save_total_limit=1,  # keep only the most recent checkpoint\n",
        "            bf16=torch.cuda.is_bf16_supported(),  # use BF16 if supported\n",
        "            fp16=not torch.cuda.is_bf16_supported() and torch.cuda.is_available(),  # fallback to FP16\n",
        "            gradient_checkpointing=True,  # save memory\n",
        "            report_to=[\"tensorboard\"],  # enable TensorBoard logging\n",
        "            optim=\"paged_adamw_8bit\" if torch.cuda.is_available() else \"adamw_torch\",  # use 8-bit optimizer on GPU\n",
        "            warmup_ratio=0.1,\n",
        "            weight_decay=0.01,\n",
        "            seed=42,\n",
        "        )\n",
        "\n",
        "        # Instantiate the Trainer class for supervised fine-tuning\n",
        "        logger.info(\"Instantiating Trainer for causal LM distillation.\")\n",
        "        trainer = Trainer(\n",
        "            model=student_model_peft,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=student_tokenizer,\n",
        "        )\n",
        "        logger.info(\"Trainer initialized.\")\n",
        "\n",
        "        # Start the training process\n",
        "        logger.info(\"--- Starting distillation training... ---\")\n",
        "        logger.info(\"Note: Training student to mimic OpenAI teacher outputs using standard cross-entropy loss.\")\n",
        "        train_start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            train_result = trainer.train()\n",
        "            logger.info(\"--- Distillation training finished ---\")\n",
        "            logger.info(f\"Train Result: {train_result}\")\n",
        "        except Exception as e:\n",
        "            logger.error(\"--- ERROR during distillation training ---\", exc_info=True)\n",
        "            train_result = None\n",
        "\n",
        "        # Log how long training took\n",
        "        train_duration = time.time() - train_start_time\n",
        "        logger.info(f\"Training loop duration: {train_duration:.2f}s\")\n",
        "\n",
        "        # Save the trained model locally if training completed\n",
        "        if train_result is not None:\n",
        "            logger.info(f\"Saving distilled adapters locally to: {LOCAL_OUTPUT_DIR}\")\n",
        "            try:\n",
        "                os.makedirs(LOCAL_OUTPUT_DIR, exist_ok=True)\n",
        "                trainer.save_model(LOCAL_OUTPUT_DIR)\n",
        "                student_tokenizer.save_pretrained(LOCAL_OUTPUT_DIR)\n",
        "                logger.info(\"Distilled adapters and tokenizer saved locally.\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error saving adapters locally: {e}\", exc_info=True)\n",
        "\n",
        "            # Optional: push the adapters to Hugging Face Hub\n",
        "            if HUB_REPO_ID_DISTILL and trainer:\n",
        "                logger.info(f\"Pushing distilled adapters to Hugging Face Hub: {HUB_REPO_ID_DISTILL}\")\n",
        "                try:\n",
        "                    api = HfApi()\n",
        "                    api.create_repo(repo_id=HUB_REPO_ID_DISTILL, exist_ok=True, private=True)\n",
        "                    api.upload_folder(\n",
        "                        folder_path=LOCAL_OUTPUT_DIR,\n",
        "                        repo_id=HUB_REPO_ID_DISTILL,\n",
        "                        repo_type=\"model\",\n",
        "                        commit_message=f\"Distillation (Phase 1 - OpenAI Teacher {OPENAI_TEACHER_MODEL})\"\n",
        "                    )\n",
        "                    logger.info(f\"Successfully pushed adapters to {HUB_REPO_ID_DISTILL}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"ERROR pushing distilled adapters to Hub: {e}\", exc_info=True)\n",
        "        else:\n",
        "            logger.warning(\"Skipping adapter saving/pushing because training did not complete successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during training setup or Trainer initialization: {e}\", exc_info=True)\n",
        "else:\n",
        "    logger.error(\"--- Skipping Training: Missing prerequisites ---\")\n",
        "\n",
        "# Cleanup to free GPU memory and avoid lingering state\n",
        "try:\n",
        "    del student_model, student_model_peft, trainer, tokenized_dataset, distillation_dataset\n",
        "except Exception:\n",
        "    pass\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    logger.info(\"Cleared CUDA cache.\")\n",
        "\n",
        "print(\"\\n--- Distillation Script Finished ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIzolxhNcJtz"
      },
      "outputs": [],
      "source": [
        "# @title 9. Evaluate the distilled model\n",
        "\n",
        "# Import required libraries for evaluation, dataset handling, model loading, etc.\n",
        "import evaluate\n",
        "import nltk\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import gc\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Define paths and constants for evaluation\n",
        "DISTILLED_ADAPTERS_PATH = LOCAL_OUTPUT_DIR\n",
        "BASE_STUDENT_MODEL_ID = STUDENT_MODEL_ID\n",
        "TEST_DATASET_PATH = \"/content/test_medical_dialogues.json\"\n",
        "EVAL_SAMPLE_LIMIT = 20\n",
        "MAX_GENERATION_TOKENS = 350\n",
        "\n",
        "# Ensure required tokenizer resource is available for metrics like BLEU, ROUGE\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    logger.info(\"NLTK 'punkt' resource found.\")\n",
        "except LookupError:\n",
        "    logger.info(\"NLTK 'punkt' tokenizer not found. Downloading...\")\n",
        "    try:\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        logger.info(\"NLTK 'punkt' downloaded successfully.\")\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "        logger.info(\"Verified NLTK 'punkt' resource after download.\")\n",
        "    except Exception as download_error:\n",
        "        logger.error(f\"Failed to download NLTK 'punkt' resource: {download_error}\", exc_info=True)\n",
        "\n",
        "# Set evaluation device to GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Evaluation using device: {device}\")\n",
        "\n",
        "# Load the test dataset from disk\n",
        "logger.info(f\"--- Loading Test Dataset from: {TEST_DATASET_PATH} ---\")\n",
        "test_data = []\n",
        "if not os.path.exists(TEST_DATASET_PATH):\n",
        "    logger.error(f\"Test dataset not found: '{TEST_DATASET_PATH}'. Cannot perform evaluation.\")\n",
        "else:\n",
        "    try:\n",
        "        with open(TEST_DATASET_PATH, 'r') as f:\n",
        "            all_data = json.load(f)\n",
        "        required_input_key = 'input'\n",
        "        required_output_key = 'output'\n",
        "\n",
        "        # Filter only entries with both input and output strings\n",
        "        for item in all_data:\n",
        "            input_val = item.get(required_input_key)\n",
        "            output_val = item.get(required_output_key)\n",
        "            if isinstance(input_val, str) and input_val and isinstance(output_val, str) and output_val:\n",
        "                test_data.append({\"input\": input_val, \"output\": output_val})\n",
        "\n",
        "        logger.info(f\"Loaded {len(all_data)} examples, filtered to {len(test_data)} with required keys.\")\n",
        "        if test_data:\n",
        "            # Cap the sample size for fast evaluation\n",
        "            test_data = test_data[:EVAL_SAMPLE_LIMIT]\n",
        "            logger.info(f\"Using {len(test_data)} samples for evaluation (limited by EVAL_SAMPLE_LIMIT).\")\n",
        "        else:\n",
        "            logger.warning(\"Test dataset loaded but no valid examples found with 'input' and 'output' keys.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading or processing test dataset: {e}\")\n",
        "        test_data = []\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "test_dataset = None\n",
        "if test_data:\n",
        "    try:\n",
        "        test_dataset = Dataset.from_list(test_data)\n",
        "        logger.info(\"Test data converted to Hugging Face Dataset.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to create test Dataset object: {e}\")\n",
        "else:\n",
        "    logger.error(\"No valid test data loaded. Evaluation cannot proceed.\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "def load_model_for_eval(model_id, adapters_path=None):\n",
        "    \"\"\"Loads a model (optionally merging adapters) for evaluation.\"\"\"\n",
        "    logger.info(f\"Loading model: {model_id}\" + (f\" with adapters from {adapters_path}\" if adapters_path else \"\"))\n",
        "    try:\n",
        "        # Load model in 4-bit for efficient inference\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        )\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.bfloat16\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "        # Ensure tokenizer and model can pad properly for batching\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            tokenizer.padding_side = \"left\"\n",
        "            model.config.pad_token_id = tokenizer.eos_token_id\n",
        "            logger.info(f\"Set pad token for {model_id}\")\n",
        "\n",
        "        # Load and merge LoRA adapters into the base model\n",
        "        if adapters_path:\n",
        "            if not os.path.isdir(adapters_path):\n",
        "                logger.error(f\"Adapter path does not exist or is not a directory: {adapters_path}\")\n",
        "                raise FileNotFoundError(f\"Adapters not found at {adapters_path}\")\n",
        "            logger.info(f\"Loading and merging PEFT adapters from {adapters_path}...\")\n",
        "            model = PeftModel.from_pretrained(model, adapters_path)\n",
        "            model = model.merge_and_unload()\n",
        "            logger.info(\"Adapters merged successfully.\")\n",
        "\n",
        "        model.eval()  # Set to eval mode for inference\n",
        "        logger.info(f\"Model {model_id}{'+Adapters' if adapters_path else ''} loaded successfully.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model {model_id}: {e}\", exc_info=True)\n",
        "        return None, None\n",
        "\n",
        "# Load the student model + distilled adapters for evaluation\n",
        "logger.info(\"--- Loading Distilled Student Model ---\")\n",
        "distilled_model, distilled_tokenizer = load_model_for_eval(\n",
        "    BASE_STUDENT_MODEL_ID,\n",
        "    adapters_path=DISTILLED_ADAPTERS_PATH\n",
        ")\n",
        "\n",
        "# (Optional placeholder for reference model â€” not used here)\n",
        "reference_model, reference_tokenizer = None, None\n",
        "\n",
        "# Log GPU memory usage to help monitor resource consumption\n",
        "if torch.cuda.is_available():\n",
        "    logger.info(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "    logger.info(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
        "\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnvJwokebnUg"
      },
      "outputs": [],
      "source": [
        "# @title 10. Generation Function\n",
        "\n",
        "import re\n",
        "\n",
        "def generate_response(model, tokenizer, prompt_text, max_new_tokens=MAX_GENERATION_TOKENS):\n",
        "    \"\"\"\n",
        "    Generates a response from a model given a prompt.\n",
        "    Includes adding instruction for <answer> block if needed.\n",
        "    Handles chat templates.\n",
        "    \"\"\"\n",
        "    if model is None or tokenizer is None:\n",
        "        return \"[ERROR: Model or Tokenizer not loaded]\"\n",
        "\n",
        "    generation_prompt_text = prompt_text\n",
        "\n",
        "    try:\n",
        "        # Attempt to format the prompt using the chat template (if available for this tokenizer)\n",
        "        messages = [{\"role\": \"user\", \"content\": generation_prompt_text}]\n",
        "        prompt_formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    except Exception as e:\n",
        "        # Fallback to raw prompt if tokenizer doesn't support chat templates\n",
        "        logger.warning(f\"Could not apply chat template for {tokenizer.name_or_path}: {e}. Using raw prompt.\")\n",
        "        prompt_formatted = generation_prompt_text\n",
        "\n",
        "    try:\n",
        "        # Ensure the input fits within the model's max sequence length\n",
        "        max_input_length = MAX_SEQ_LENGTH - max_new_tokens - 5\n",
        "        inputs = tokenizer(\n",
        "            prompt_formatted,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_input_length\n",
        "        ).to(model.device)\n",
        "\n",
        "        # Guard against prompt tokenization resulting in an empty sequence\n",
        "        if inputs['input_ids'].shape[1] == 0:\n",
        "            logger.warning(f\"Input tokens resulted in empty sequence for prompt: {generation_prompt_text[:100]}...\")\n",
        "            return \"[ERROR: Empty token sequence]\"\n",
        "\n",
        "        # Generation configuration: top-p sampling with temperature\n",
        "        gen_kwargs = {\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"temperature\": 0.6,\n",
        "            \"top_p\": 0.9,\n",
        "            \"do_sample\": True,\n",
        "            \"pad_token_id\": tokenizer.pad_token_id,\n",
        "            \"eos_token_id\": tokenizer.eos_token_id,\n",
        "        }\n",
        "\n",
        "        # Generate text without tracking gradients\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "        # Strip the prompt tokens and decode only the generated part\n",
        "        output_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "        response = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return response.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during generation with {tokenizer.name_or_path}: {e}\", exc_info=True)\n",
        "        return f\"[ERROR: Generation failed - {e}]\"\n",
        "\n",
        "# Run a quick sanity check on generation if model and data are ready\n",
        "if distilled_model and distilled_tokenizer and test_dataset and len(test_dataset) > 0:\n",
        "    logger.info(\"--- Testing generation (distilled model) ---\")\n",
        "    sample_prompt_test = test_dataset[0]['input']\n",
        "    sample_output_test = generate_response(distilled_model, distilled_tokenizer, sample_prompt_test)\n",
        "    logger.info(f\"Sample Prompt:\\n{sample_prompt_test}\")\n",
        "    logger.info(f\"Sample Distilled Output:\\n{sample_output_test}\")\n",
        "\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB_63-gLbuaK"
      },
      "outputs": [],
      "source": [
        "# @title 11. Run Evaluation Loop and Calculate Metrics\n",
        "\n",
        "import re\n",
        "import evaluate\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "import logging\n",
        "\n",
        "def extract_answer(text):\n",
        "    \"\"\"Extracts text content from within <answer>...</answer> tags.\"\"\"\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    match = re.search(r'<answer>(.*?)</answer>', text, re.IGNORECASE | re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    else:\n",
        "        return text.strip()\n",
        "\n",
        "logger.info(\"--- Loading Evaluation Metrics (BLEU, ROUGE) ---\")\n",
        "try:\n",
        "    bleu_metric = evaluate.load(\"bleu\")\n",
        "    rouge_metric = evaluate.load(\"rouge\")\n",
        "    metrics_loaded = True\n",
        "    logger.info(\"Metrics loaded successfully.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load evaluation metrics: {e}\", exc_info=True)\n",
        "    metrics_loaded = False\n",
        "\n",
        "# Container for storing both raw and processed generation results\n",
        "results = {\n",
        "    \"distilled\": {\"predictions\": [], \"references\": [], \"processed_predictions\": [], \"processed_references\": []},\n",
        "}\n",
        "\n",
        "# Only proceed if we have metrics + model + data\n",
        "if metrics_loaded and test_dataset and distilled_model and distilled_tokenizer:\n",
        "    logger.info(f\"--- Running Evaluation on {len(test_dataset)} samples (Distilled Model Only) ---\")\n",
        "    for example in tqdm(test_dataset, desc=\"Evaluating Distilled Model\"):\n",
        "        prompt = example['input']\n",
        "        raw_reference_text = example['output']\n",
        "\n",
        "        # Extract clean reference text (within <answer> tags if present)\n",
        "        processed_reference = extract_answer(raw_reference_text)\n",
        "        results[\"distilled\"][\"references\"].append(raw_reference_text)\n",
        "        results[\"distilled\"][\"processed_references\"].append(processed_reference)\n",
        "\n",
        "        # Generate model prediction and clean the response\n",
        "        raw_distilled_pred = generate_response(distilled_model, distilled_tokenizer, prompt)\n",
        "        processed_distilled_pred = extract_answer(raw_distilled_pred)\n",
        "        results[\"distilled\"][\"predictions\"].append(raw_distilled_pred)\n",
        "        results[\"distilled\"][\"processed_predictions\"].append(processed_distilled_pred)\n",
        "\n",
        "    logger.info(\"--- Evaluation Generation Complete ---\")\n",
        "    logger.info(\"--- Calculating Metrics (using extracted <answer> content) ---\")\n",
        "\n",
        "    final_scores = {}\n",
        "    model_name = \"distilled\"\n",
        "\n",
        "    predictions = results[model_name][\"processed_predictions\"]\n",
        "    references = results[model_name][\"processed_references\"]\n",
        "\n",
        "    # Ensure matching lengths before metric calculation\n",
        "    if not predictions or not references or len(predictions) != len(references):\n",
        "        logger.error(f\"Mismatch or empty lists for processed {model_name}. Cannot calculate scores.\")\n",
        "        final_scores[model_name] = {\"error\": \"Invalid processed prediction/reference data\"}\n",
        "    else:\n",
        "        predictions = [str(p) for p in predictions]\n",
        "        references = [str(r) for r in references]\n",
        "\n",
        "        try:\n",
        "            # BLEU expects a list of lists of references\n",
        "            bleu_references_formatted = [[ref] for ref in references]\n",
        "            bleu_score = bleu_metric.compute(predictions=predictions, references=bleu_references_formatted)\n",
        "            logger.info(f\"{model_name.capitalize()} BLEU Score: {bleu_score}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating BLEU for {model_name}: {e}\", exc_info=True)\n",
        "            bleu_score = {\"bleu\": \"Error\"}\n",
        "\n",
        "        try:\n",
        "            rouge_score = rouge_metric.compute(predictions=predictions, references=references, use_aggregator=True)\n",
        "            logger.info(f\"{model_name.capitalize()} ROUGE Scores: {rouge_score}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating ROUGE for {model_name}: {e}\", exc_info=True)\n",
        "            rouge_score = {\"rouge1\": \"Error\", \"rouge2\": \"Error\", \"rougeL\": \"Error\", \"rougeLsum\": \"Error\"}\n",
        "\n",
        "        final_scores[model_name] = {**bleu_score, **rouge_score}\n",
        "\n",
        "    # Print the final evaluation metrics\n",
        "    print(\"\\n--- Evaluation Results (Based on <answer> content) ---\")\n",
        "    print(f\"Evaluated on {len(test_dataset)} samples.\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    if model_name in final_scores:\n",
        "        scores = final_scores[model_name]\n",
        "        print(f\"\\n>> {model_name.capitalize()} Model ({BASE_STUDENT_MODEL_ID}+Adapters):\")\n",
        "        if \"error\" in scores:\n",
        "            print(f\"  Error calculating scores: {scores['error']}\")\n",
        "        else:\n",
        "            bleu_val = scores.get('bleu')\n",
        "            r1_val = scores.get('rouge1')\n",
        "            r2_val = scores.get('rouge2')\n",
        "            rl_val = scores.get('rougeL')\n",
        "            print(f\"  BLEU: {bleu_val:.4f}\" if isinstance(bleu_val, (int, float)) else f\"  BLEU: {bleu_val}\")\n",
        "            print(f\"  ROUGE-1: {r1_val:.4f}\" if isinstance(r1_val, (int, float)) else f\"  ROUGE-1: {r1_val}\")\n",
        "            print(f\"  ROUGE-2: {r2_val:.4f}\" if isinstance(r2_val, (int, float)) else f\"  ROUGE-2: {r2_val}\")\n",
        "            print(f\"  ROUGE-L: {rl_val:.4f}\" if isinstance(rl_val, (int, float)) else f\"  ROUGE-L: {rl_val}\")\n",
        "            print(\"-\" * 25)\n",
        "    else:\n",
        "        print(f\"\\n>> No scores calculated for {model_name.capitalize()} Model.\")\n",
        "\n",
        "    # Display one full example of generation and comparison\n",
        "    if len(test_dataset) > 0 and \"distilled\" in results:\n",
        "        print(\"\\n--- Example Generations (Sample 0) ---\")\n",
        "        print(f\"Prompt:\\n{test_dataset[0]['input']}\\n\")\n",
        "        print(f\"Ground Truth (Raw):\\n{results['distilled']['references'][0]}\\n\")\n",
        "        print(f\"Ground Truth (Processed <answer>):\\n{results['distilled']['processed_references'][0]}\\n\")\n",
        "        print(f\"Distilled Model (Raw):\\n{results['distilled']['predictions'][0]}\\n\")\n",
        "        print(f\"Distilled Model (Processed <answer>):\\n{results['distilled']['processed_predictions'][0]}\\n\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "else:\n",
        "    logger.error(\"Evaluation skipped due to missing prerequisites (metrics, test data, or distilled model/tokenizer).\")\n",
        "\n",
        "# Free memory and clear CUDA cache\n",
        "logger.info(\"Cleaning up evaluation models...\")\n",
        "try:\n",
        "    if 'distilled_model' in locals(): del distilled_model\n",
        "    if 'distilled_tokenizer' in locals(): del distilled_tokenizer\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    logger.info(\"Cleared CUDA cache after evaluation.\")\n",
        "\n",
        "print(\"\\n--- Evaluation Script Finished ---\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
